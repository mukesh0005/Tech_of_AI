{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "The following function generates a sequence of n Fibonacci numbers (not counting the starting two values). If scale_data is set to True, then it would also use the MinMaxScaler from scikit-learn to scale the values between 0 and 1. Letâ€™s see its output for n=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def get_fib_seq(n, scale_data=True):\n",
    "    # Get the Fibonacci sequence\n",
    "    seq = np.zeros(n)\n",
    "    fib_n1 = 0.0\n",
    "    fib_n = 1.0 \n",
    "    for i in range(n):\n",
    "            seq[i] = fib_n1 + fib_n\n",
    "            fib_n1 = fib_n\n",
    "            fib_n = seq[i] \n",
    "    scaler = []\n",
    "    if scale_data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        seq = np.reshape(seq, (n, 1))\n",
    "        seq = scaler.fit_transform(seq).flatten()        \n",
    "    return seq, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  5.  8. 13. 21. 34. 55. 89.]\n"
     ]
    }
   ],
   "source": [
    "fib_seq = get_fib_seq(10, False)[0]\n",
    "print(fib_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function get_fib_XY() that reformats the sequence into training examples and target values to be used by the Keras input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time_steps is number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
    "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n",
    "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[0:rows_x]\n",
    "    for i in range(time_steps-1):\n",
    "        temp = dat[i+1:rows_x+i+1]\n",
    "        X = np.column_stack((X, temp))\n",
    "    # random permutation with fixed seed   \n",
    "    rand = np.random.RandomState(seed=13)\n",
    "    idx = rand.permutation(rows_x)\n",
    "    split = int(train_percent*rows_x)\n",
    "    train_ind = idx[0:split]\n",
    "    test_ind = idx[split:]\n",
    "    trainX = X[train_ind]\n",
    "    trainY = Y[train_ind]\n",
    "    testX = X[test_ind]\n",
    "    testY = Y[test_ind]\n",
    "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n",
    "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
    "    return trainX, trainY, testX, testY, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "time_steps = 20\n",
    "hidden_units = 2\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a traditional RNN network\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n",
    "                   activation=['tanh', 'tanh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "826/826 - 3s - loss: 0.0020 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 0.0016 - 2s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 2s - loss: 0.0011 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 2s - loss: 9.8351e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 9.1950e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 8.5070e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 7.6599e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 3s - loss: 7.0726e-04 - 3s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 3s - loss: 6.3528e-04 - 3s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 5.5091e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 4.8420e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 4.1512e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 3.5407e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 3.0404e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 2.5165e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 2.0243e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 1.7466e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 1.4926e-04 - 2s/epoch - 3ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 2s - loss: 1.3136e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 1.2662e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 1.1379e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 1.0506e-04 - 2s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 9.2916e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 6.8440e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 6.0224e-05 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 5.9036e-05 - 2s/epoch - 2ms/step\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 3.9706e-05\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 6.2636e-06\n",
      "Train set MSE =  3.970576653955504e-05\n",
      "Test set MSE =  6.263581781240646e-06\n"
     ]
    }
   ],
   "source": [
    "# Generate the dataset for the network\n",
    "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n",
    "# Train the network\n",
    "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    "\n",
    "\n",
    "# Evalute model\n",
    "train_mse = model_RNN.evaluate(trainX, trainY)\n",
    "test_mse = model_RNN.evaluate(testX, testY)\n",
    "\n",
    "# Print error\n",
    "print(\"Train set MSE = \", train_mse)\n",
    "print(\"Test set MSE = \", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e) \n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
      "                                                                 \n",
      " attention (attention)       (None, 2)                 22        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33 (132.00 Byte)\n",
      "Trainable params: 33 (132.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "826/826 - 4s - loss: 0.0016 - 4s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 2s - loss: 0.0016 - 2s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 2s - loss: 0.0016 - 2s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 2s - loss: 0.0015 - 2s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 2s - loss: 0.0014 - 2s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 2s - loss: 0.0013 - 2s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 2s - loss: 0.0012 - 2s/epoch - 2ms/step\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Train set MSE with attention =  0.0011574450181797147\n",
      "Test set MSE with attention =  0.0010295144747942686\n"
     ]
    }
   ],
   "source": [
    "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
    "    x=Input(shape=input_shape)\n",
    "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    attention_layer = attention()(RNN_layer)\n",
    "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
    "    model=Model(x,outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')    \n",
    "    return model    \n",
    "\n",
    "# Create the model with attention, train and evaluate\n",
    "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n",
    "                                  input_shape=(time_steps,1), activation='tanh')\n",
    "model_attention.summary()    \n",
    "\n",
    "\n",
    "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    "\n",
    "# Evalute model\n",
    "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
    "test_mse_attn = model_attention.evaluate(testX, testY)\n",
    "\n",
    "# Print error\n",
    "print(\"Train set MSE with attention = \", train_mse_attn)\n",
    "print(\"Test set MSE with attention = \", test_mse_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
